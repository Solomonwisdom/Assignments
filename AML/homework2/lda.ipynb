{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 2.648s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\\\n",
    "\n",
    "import os\n",
    "\n",
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([\"{0:s}: {1:.2f}%\".format(feature_names[i], topic[i]*100/topic.sum())\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "data_samples = []\n",
    "if os.path.isfile(\"processed_news.txt\"):\n",
    "    with open(\"processed_news.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            data_samples.append(line)\n",
    "else:\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    stopworddic = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "    # to filter out useless terms early on: the posts are stripped of headers,\n",
    "    # footers and quoted replies, and common English words, words occurring in\n",
    "    # only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    t0 = time()\n",
    "    # set flag to allow verbose regexps\n",
    "    # abbreviations, e.g. U.S.A.\n",
    "    # numbers, incl. currency and percentages\n",
    "    # words w/ optional internal hyphens/apostrophe\n",
    "    # ellipsis\n",
    "    # special characters with meanings\n",
    "    pattern = r\"\"\"(?x)\n",
    "    (?:[A-Z]\\.)+\n",
    "    |\\d+(?:\\.\\d+)?%?\n",
    "    |\\w+(?:[-']\\w+)*\n",
    "    |\\.\\.\\.\n",
    "    |(?:[.,;\"'?():-_`])\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def is_english(word):\n",
    "        flag = True\n",
    "        for uchar in word:\n",
    "            if uchar > u'\\u007f':\n",
    "                flag = False\n",
    "        return flag\n",
    "\n",
    "\n",
    "    with open(\"news.txt\", \"r\") as f, open(\"processed_news.txt\", \"w\") as pf:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = nltk.regexp_tokenize(line.strip(), pattern)\n",
    "            words = [wordnet_lemmatizer.lemmatize(token.lower())\n",
    "                     for token in tokens if is_english(token.lower()) and token.lower() not in stopworddic]\n",
    "            if len(words) > 0:\n",
    "                data_samples.append(\" \".join(words))\n",
    "                pf.write(\" \".join(words)+'\\n')\n",
    "            # if i % 1000 == 0:\n",
    "            #     print(i)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8883, 87920)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2428850,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
